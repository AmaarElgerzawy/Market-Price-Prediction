# -*- coding: utf-8 -*-
"""Copy of GraduationProject_ME.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lSoG9YCr0rUIUnhMSdLfuCPCn934W5Ep
"""

# !pip install --upgrade --no-cache-dir git+https://github.com/rongardF/tvdatafeed.git
# !pip install --upgrade pip setuptools wheel
# !pip install ta

import numpy as np
from tvDatafeed import TvDatafeed, Interval
import pandas as pd
import ta
from datetime import datetime, timedelta

period = Interval.in_daily
pair = 'EURUSD'
broker = 'OANDA'

tv = TvDatafeed()
df = tv.get_hist(symbol=pair, exchange=broker, interval=period, n_bars=10000, extended_session=True)

my_dict = df.to_dict()

df['close'] = [value for value in df['close'] if not np.isnan(value)]
df['open'] = [value for value in df['open'] if not np.isnan(value)]
df['low'] = [value for value in df['low'] if not np.isnan(value)]
df['high'] = [value for value in df['high'] if not np.isnan(value)]
df['volume'] = [value for value in df['volume'] if not np.isnan(value)]

# df['date_column'] = pd.to_datetime(df.index)
# df.set_index('date_column', inplace=True)
# cutoff_date = pd.to_datetime('2023-5-31')
# df = df[df.index <= cutoff_date]

dxy = tv.get_hist(symbol='DXY', exchange='TVC', interval=period, n_bars=10000, extended_session=True)
dxy['close'] = [value for value in dxy['close'] if not np.isnan(value)]

df = df[(df['close'] - df['close'].mean())/ df['close'].std() < 3]
df = df[(df['open'] - df['open'].mean())/ df['open'].std() < 3]
df = df[(df['low'] - df['low'].mean())/ df['low'].std() < 3]
df = df[(df['high'] - df['high'].mean())/ df['high'].std() < 3]

dxy = dxy[dxy.index.isin(df.index)]

def count_nan(data):
  c = 0
  for i in data:
    if i == np.nan:
      c += 1
  return c

# data = pd.read_csv('/content/sentiment_annotated_with_texts.csv')
# data = data[data['ticker'] == 'EURUSD'].reset_index()

# droping = [x for x in data.columns if x not in ['published_at','finbert_sent_score']]
# data = data.drop(columns=droping)

# format_str = '%Y-%m-%d %H:%M:%S'

# importance = np.zeros(len(df['close']))
# times = []
# i = 0

# if period == Interval.in_1_hour:
#   while i < len(data):
#     if data['published_at'][i] != None:
#       data.loc[i, 'published_at'] = datetime.strptime(str(data.loc[i, 'published_at']), format_str) + timedelta(hours=10)
#     i += 1

#   data['news_times'] = list(map(lambda a: a.replace(minute=0, second=0) ,data['published_at']))
#   data.groupby('published_at').mean()

#   times = list(my_dict['close'].keys())
#   for i,val in enumerate(times):
#     times[i] = datetime(val.year, val.month, val.day, val.hour)

#   common_elements = [(i, j, x) for i, x in enumerate(data['news_times']) if x in times for j, y in enumerate(times) if y == x]

#   for i,j,val in common_elements:
#     importance[j] = data['finbert_sent_score'][i]

# elif period == Interval.in_daily:
#   while i < len(data):
#     if data['published_at'][i] != None:
#       data.loc[i, 'published_at'] = datetime.strptime(str(data.loc[i, 'published_at']), format_str).replace(hour=0,minute=0,second=0)
#     i += 1

#   data['news_times'] = list(map(lambda a: a.replace(hour=0,minute=0, second=0) ,data['published_at']))
#   data.groupby('published_at').mean()

#   times = list(my_dict['close'].keys())
#   for i,val in enumerate(times):
#     times[i] = datetime(val.year, val.month, val.day)

#   common_elements = [(i, j, x) for i, x in enumerate(data['news_times']) if x in times for j, y in enumerate(times) if y == x]

#   for i,j,val in common_elements:
#     importance[j] = data['finbert_sent_score'][i]

sma21 = ta.trend.sma_indicator(df['close'],21).iloc[:].values
sma38 = ta.trend.sma_indicator(df['close'],38).iloc[:].values
sma50 = ta.trend.sma_indicator(df['close'],50).iloc[:].values
sma100 = ta.trend.sma_indicator(df['close'],100).iloc[:].values
sma200 = ta.trend.sma_indicator(df['close'],200).iloc[:].values

close = df['close']

all_features = ta.add_all_ta_features(df,'open','high','low','close','volume')

to_drop = ['symbol']
for col in all_features.columns:
  if count_nan(all_features[col]) > 1000:
    to_drop.append(col)
all_features = all_features.drop(columns=to_drop)

# all_features['news'] = importance
all_features['sma21'] = sma21
all_features['sma38'] = sma38
all_features['sma50'] = sma50
all_features['sma100'] = sma100
all_features['sma200'] = sma200

df = df[~(df == 0).any(axis=1)]

c1 = max([count_nan(all_features[indicator]) for indicator in all_features.columns])
clipper = c1 + len(all_features['close'][c1:])%100

all_features = all_features[clipper:]
all_features = all_features.dropna(axis='columns')

# all_features.to_csv('all_features.csv')

def create_sequences_multistep(df, sequence_length=50, prediction_length=5):
    num_sequences = len(df) - sequence_length - prediction_length + 1

    X = np.empty((num_sequences, sequence_length, df.shape[1]), dtype=df.values.dtype)
    y = np.empty((num_sequences, prediction_length), dtype=df[df.columns[0]].values.dtype)

    for i in range(num_sequences):
      X[i] = df.values[i:(i + sequence_length)]
      y[i] = df[df.columns[0]].values[(i + sequence_length):(i + sequence_length + prediction_length)]

    return X, y

sequence_length = 100
prediction_length = 10

df = all_features
df = df.replace([np.inf, -np.inf], np.nan)
df = df.dropna(axis=1, how='any')

X1, y1 = create_sequences_multistep(df, sequence_length, prediction_length)

from sklearn.model_selection import train_test_split

_, X_test, _, y_test = train_test_split(X1, y1, test_size=0.2, shuffle=False)

permutation = np.random.permutation(len(X1))

np.append(X1,X1[permutation])
np.append(y1,y1[permutation])

permutation = np.random.permutation(len(X1))

X2,y2 = X1[permutation],y1[permutation]

X_train, _, y_train, _ = train_test_split(X2, y2, test_size=0.001, shuffle=True)

from sklearn.preprocessing import MinMaxScaler,StandardScaler
import numpy as np

f_scaler = MinMaxScaler(feature_range=(-1, 1))

X_train_scaled = f_scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)
X_test_scaled = f_scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)
X1 = f_scaler.transform(X1.reshape(-1, X1.shape[-1])).reshape(X1.shape)

f_y_scaler = MinMaxScaler(feature_range=(-1, 1))
y_train_scaled = f_y_scaler.fit_transform(y_train)
y_test_scaled = f_y_scaler.transform(y_test)

from keras.models import Sequential
from keras.layers import Conv1D, MaxPooling1D, Flatten, Dropout, Dense,LSTM
from tensorflow.keras import optimizers ,activations, metrics

optimizer = optimizers.Adam(learning_rate=0.001)
activation = activations.tanh

model_ann = Sequential([
    Flatten(input_shape=(X_train_scaled.shape[1], X_train_scaled.shape[2])),
    Dense(128, activation=activation),
    Dropout(0.2),
    Dense(y_train_scaled.shape[1])
])

model_ann.compile(optimizer=optimizer, loss='mean_squared_error',metrics=['accuracy'])

history_ann = model_ann.fit(X_train_scaled, y_train_scaled, epochs=25, validation_split=0.1, shuffle=True)

predicted_prices_scaled_ann = model_ann.predict(X_test_scaled)

predicted_prices_ann = f_y_scaler.inverse_transform(predicted_prices_scaled_ann)

model_cnn = Sequential([
    Conv1D(filters=64, kernel_size=4, activation=activation, input_shape=(X_train_scaled.shape[1], X_train_scaled.shape[2])),
    MaxPooling1D(pool_size=2),
    Flatten(),
    Dropout(0.2),
    Dense(y_train_scaled.shape[1])  # Adjust based on how many outputs you're predicting
])

optimizer = optimizers.Adam(learning_rate=0.001)
model_cnn.compile(optimizer=optimizer, loss='mean_squared_error',metrics=['accuracy'])

history_cnn = model_cnn.fit(X_train_scaled, y_train_scaled, epochs=25, validation_split=0.1, shuffle=True)

predicted_prices_scaled_cnn = model_cnn.predict(X_test_scaled)

predicted_prices_cnn = f_y_scaler.inverse_transform(predicted_prices_scaled_cnn)

model_lstm = Sequential([
    LSTM(100, activation=activation, input_shape=(X_train_scaled.shape[1], X_train_scaled.shape[2])),
    Dropout(0.2),
    Dense(y_train_scaled.shape[1])  # Adjust based on how many outputs you're predicting
])

optimizer = optimizers.Adam(learning_rate=0.001)
model_lstm.compile(optimizer=optimizer, loss='mean_squared_error',metrics=['accuracy'])

history = model_lstm.fit(X_train_scaled, y_train_scaled, epochs=25, validation_split=0.1, shuffle=True)

predicted_prices_scaled = model_lstm.predict(X_test_scaled)

predicted_prices_lstm = f_y_scaler.inverse_transform(predicted_prices_scaled)

import matplotlib.pyplot as plt

def flatten(lst):
    flattened = []
    for item in lst:
        if isinstance(item, (list,np.ndarray)):
            flattened.extend(flatten(item))
        else:
            flattened.append(item)
    return np.array(flattened)

def calculate_sign_percentage(arr1, arr2):
    if len(arr1) != len(arr2):
        raise ValueError("Arrays must be of equal length")

    subs = [[abs(x-y) for x,y in zip(a,b)] for a,b in zip(arr1,arr2)]

    avg_single = [sum(x)/len(x) for x in subs]

    subs = flatten(subs)
    avg_err = sum(subs)/len(subs)

    min_sub = min(subs)
    max_sub = max(subs)

    i_min = int(np.argmin(subs)/10)
    i_max = int(np.argmax(subs)/10)

    min_max = [arr1[i_min],arr2[i_min],arr1[i_max],arr2[i_max]]

    return avg_err,min_sub,max_sub,avg_single,min_max

avg_err,min_sub,max_sub,avg_single,min_max = calculate_sign_percentage(y_test, predicted_prices_ann)
plt.plot(avg_single)
plt.show()

print(f"\n\nAverage Error : {avg_err}")
print(f"Minimum Error : {min_sub}")
print(f"Maximum Error : {max_sub}")

sample = X_test_scaled[0:1]
real = y_test[0]

predicted_sample_scaled = model_ann.predict(sample)
predicted_sample = f_y_scaler.inverse_transform(predicted_sample_scaled)

x = 0
labels = [[],[]]
while x < len(real)-1:
  if real[x] < real[x+1]:
    labels[0].append('up')
  else:
    labels[0].append('down')

  if predicted_sample[0][x] < predicted_sample[0][x+1]:
    labels[1].append('up')
  else:
    labels[1].append('down')

  x+=1

acc = 0
for i,j,l,m in zip(real,predicted_sample[0],labels[0],labels[1]):
  print(i,j,sep='  ---->  ',end='    ')
  print(l,m,sep='  ---->  ')
  if l == m:
    acc += 1

print(f'{acc}/{len(labels[0])}')

print('\n\n\n')

for i,j,l,m in zip(min_max[0],min_max[1],min_max[2],min_max[3]):
  print(i,j,abs(i-j),sep='  ---->  ',end='    ')
  print(l,m,abs(l-m),sep='  ---->  ')

avg_err,min_sub,max_sub,avg_single,min_max = calculate_sign_percentage(y_test, predicted_prices_cnn)
plt.plot(avg_single)
plt.show()

print(f"\n\nAverage Error : {avg_err}")
print(f"Minimum Error : {min_sub}")
print(f"Maximum Error : {max_sub}")

sample = X_test_scaled[0:1]
real = y_test[0]

predicted_sample_scaled = model_cnn.predict(sample)
predicted_sample = f_y_scaler.inverse_transform(predicted_sample_scaled)

x = 0
labels = [[],[]]
while x < len(real)-1:
  if real[x] < real[x+1]:
    labels[0].append('up')
  else:
    labels[0].append('down')

  if predicted_sample[0][x] < predicted_sample[0][x+1]:
    labels[1].append('up')
  else:
    labels[1].append('down')

  x+=1

acc = 0
for i,j,l,m in zip(real,predicted_sample[0],labels[0],labels[1]):
  print(i,j,sep='  ---->  ',end='    ')
  print(l,m,sep='  ---->  ')
  if l == m:
    acc += 1

print(f'{acc}/{len(labels[0])}')

print('\n\n\n')

for i,j,l,m in zip(min_max[0],min_max[1],min_max[2],min_max[3]):
  print(i,j,abs(i-j),sep='  ---->  ',end='    ')
  print(l,m,abs(l-m),sep='  ---->  ')

avg_err,min_sub,max_sub,avg_single,min_max = calculate_sign_percentage(y_test, predicted_prices_lstm)
plt.plot(avg_single)
plt.show()

print(f"\n\nAverage Error : {avg_err}")
print(f"Minimum Error : {min_sub}")
print(f"Maximum Error : {max_sub}")

sample = X_test_scaled[0:1]
real = y_test[0]

predicted_sample_scaled = model_lstm.predict(sample)
predicted_sample = f_y_scaler.inverse_transform(predicted_sample_scaled)

x = 0
labels = [[],[]]
while x < len(real)-1:
  if real[x] < real[x+1]:
    labels[0].append('up')
  else:
    labels[0].append('down')

  if predicted_sample[0][x] < predicted_sample[0][x+1]:
    labels[1].append('up')
  else:
    labels[1].append('down')

  x+=1

acc = 0
for i,j,l,m in zip(real,predicted_sample[0],labels[0],labels[1]):
  print(i,j,sep='  ---->  ',end='    ')
  print(l,m,sep='  ---->  ')
  if l == m:
    acc += 1

print(f'{acc}/{len(labels[0])}')

print('\n\n\n')

for i,j,l,m in zip(min_max[0],min_max[1],min_max[2],min_max[3]):
  print(i,j,abs(i-j),sep='  ---->  ',end='    ')
  print(l,m,abs(l-m),sep='  ---->  ')

def make_direction(data):
  directions = np.zeros_like(data)
  directions[:, :-1] = np.where(data[:, :-1] < data[:, 1:], 1, -1)
  directions = directions[:, :-1]
  return directions

direction_ann = make_direction(predicted_prices_ann)
direction_cnn = make_direction(predicted_prices_cnn)
direction_lstm = make_direction(predicted_prices_lstm)

predicted_prices_ann = predicted_prices_ann[:, 1:]
predicted_prices_cnn = predicted_prices_cnn[:, 1:]
predicted_prices_lstm = predicted_prices_lstm[:, 1:]

new_input = []

for ann, cnn, lstm, d_ann, d_cnn, d_lstm in zip(predicted_prices_ann, predicted_prices_cnn, predicted_prices_lstm, direction_ann, direction_cnn, direction_lstm):
  x = []
  for j in range(9):
    x.append([ann[j], cnn[j], lstm[j], d_ann[j], d_cnn[j], d_lstm[j]])
  new_input.append(x)

new_output = y_test[:,:-1]
new_input = np.array(new_input)

print(new_input.shape)
print(new_output.shape)

# Initialize the model
model = Sequential()
model.add(LSTM(50, return_sequences=True, input_shape=(9, 6)))  # input_shape matches the time steps and features
model.add(LSTM(50))
model.add(Dense(9, activation='sigmoid'))  # Assuming binary classification

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy','mae',metrics.R2Score()])

# Model summary
model.summary()

# Split the data
X_train, X_test, y_train, y_test = train_test_split(new_input, new_output, test_size=0.2, random_state=42,shuffle=False)
X_train, _, y_train, _ = train_test_split(new_input, new_output, test_size=0.2, random_state=42)

scaler = MinMaxScaler(feature_range=(0, 1))

X_train_scaled = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)
X_test_scaled = scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)

y_scaler = MinMaxScaler(feature_range=(0, 1))
y_train_scaled = y_scaler.fit_transform(y_train)
y_test_scaled = y_scaler.transform(y_test)

# Train the model
history = model.fit(X_train_scaled, y_train_scaled, epochs=25, validation_data=(X_test_scaled, y_test_scaled))

# Evaluate the model
performance = model.evaluate(X_test_scaled, y_test_scaled)
print(f'Test Loss: {performance[0]}, Test Accuracy: {performance[1]}')

final_model_predicted_scaled = model.predict(X_test_scaled)
final_model_predicted = y_scaler.inverse_transform(final_model_predicted_scaled)

avg_err,min_sub,max_sub,avg_single,min_max = calculate_sign_percentage(y_test, final_model_predicted)
plt.plot(avg_single)
plt.show()

print(f"\n\nAverage Error : {avg_err}")
print(f"Minimum Error : {min_sub}")
print(f"Maximum Error : {max_sub}")

sample = X_test[0:1]
real = y_test[0]

predicted_sample_scaled = model.predict(sample)
predicted_sample = y_scaler.inverse_transform(final_model_predicted_scaled)

x = 0
labels = [[],[]]
while x < len(real)-1:
  if real[x] < real[x+1]:
    labels[0].append('up')
  else:
    labels[0].append('down')

  if predicted_sample[0][x] < predicted_sample[0][x+1]:
    labels[1].append('up')
  else:
    labels[1].append('down')

  x+=1

acc = 0
for i,j,l,m in zip(real,predicted_sample[0],labels[0],labels[1]):
  print(i,j,sep='  ---->  ',end='    ')
  print(l,m,sep='  ---->  ')
  if l == m:
    acc += 1

print(f'{acc}/{len(labels[0])}')

print('\n\n\n')

for i,j,l,m in zip(min_max[0],min_max[1],min_max[2],min_max[3]):
  print(i,j,abs(i-j),sep='  ---->  ',end='    ')
  print(l,m,abs(l-m),sep='  ---->  ')

X1_ann = f_y_scaler.inverse_transform(model_ann.predict(X1))
X1_cnn = f_y_scaler.inverse_transform(model_cnn.predict(X1))
X1_lstm = f_y_scaler.inverse_transform(model_lstm.predict(X1))

d_ann = make_direction(X1_ann)
d_cnn = make_direction(X1_cnn)
d_lstm = make_direction(X1_lstm)

printing = []

for ann, cnn, lstm, d_ann, d_cnn, d_lstm in zip(X1_ann, X1_cnn, X1_lstm, d_ann, d_cnn, d_lstm):
  x = []
  for j in range(9):
    x.append([ann[j], cnn[j], lstm[j], d_ann[j], d_cnn[j], d_lstm[j]])
  printing.append(x)

printing = np.array(printing)
printing = scaler.transform(printing.reshape(-1, printing.shape[-1])).reshape(printing.shape)

last_model = y_scaler.inverse_transform(model.predict(printing))

len(close),len(X1)

# Import necessary modules
import plotly.graph_objects as go

# Prepare data
flat_1 = list(map(lambda a: a[0], last_model))[1200:]
flat_2 = list(map(lambda a: a[0], y1))[1200:]

# Generate x-axis values
x = [i for i in range(len(close))]

trace1 = go.Scatter(x=x, y=flat_1, name='predicted')
trace2 = go.Scatter(x=x, y=flat_2, name='real')

# Create the figure
fig = go.Figure()

# Add traces
fig.add_trace(trace1)
fig.add_trace(trace2)

# Display the figure
fig.show()

import matplotlib.pyplot as plt

# Assuming you have a list or array of loss values
loss_values = history_ann.history['loss']

# Create a list of epochs or iterations corresponding to the loss values
epochs = list(range(1, len(loss_values) + 1))

# Create the plot
plt.plot(epochs, loss_values, 'b-')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Model Loss over Epochs')
plt.grid(True)

# Display the plot
plt.show()

import matplotlib.pyplot as plt

# Assuming you have a list or array of loss values
loss_values = history_ann.history['val_loss']

# Create a list of epochs or iterations corresponding to the loss values
epochs = list(range(1, len(loss_values) + 1))

# Create the plot
plt.plot(epochs, loss_values, 'b-')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Model Loss over Epochs')
plt.grid(True)

# Display the plot
plt.show()

#get data feed (dataset)
from tvDatafeed import TvDatafeed, Interval
import os
import pandas as pd
#in_daily

csv_file_path = "./tvdatafeed_data.csv"
tv = TvDatafeed()

df = tv.get_hist(symbol=pair, exchange=broker, interval=period, n_bars=10000, extended_session=True)
df['date_column'] = pd.to_datetime(df.index)
df.set_index('date_column', inplace=True)

# cutoff_date = pd.to_datetime('2023-5-31')
# df = df[df.index <= cutoff_date]
df['date_column'] = df.index
df.to_csv(csv_file_path, index=False)


print("Data written to", csv_file_path)

import pandas as pd
import csv

data=[]
df = pd.DataFrame(df).iloc[:,4].values
for i in range(0,len(df)-100):
  data.append(df[i:i+100])

csv_file_path = 'f.csv'

with open(csv_file_path, 'w', newline='') as csv_file:
    csv_writer = csv.writer(csv_file)
    for row in data:
        csv_writer.writerow(row)

data1 = []
def read_csv(csv_file_path):
    with open(csv_file_path, 'r') as csvfile:
        reader = csv.DictReader(csvfile)
        for row in reader:
            data1.append(row)
    return data1
df1 = pd.read_csv(csv_file_path)

row_labels = [f'Row_{i}' for i in range(len(df1))]
column_labels = [f'Column_{i}' for i in range(df1.shape[1])]
indexed_df = pd.DataFrame(data=df1.values, index=row_labels, columns=column_labels)

import pandas as pd

csv_file_path = 'f.csv'
df = pd.read_csv(csv_file_path)
df.columns = range(len(df.columns))
df.to_csv(csv_file_path, index=False)

import pandas as pd

csv_file_path = 'f.csv'
df = pd.read_csv('f.csv')

X = df.iloc[:, :50]
Y = df.iloc[:, 50:]
X = pd.DataFrame(X)
Y = pd.DataFrame(Y)

import matplotlib.pyplot as plt
import numpy as np
q_80 = int(len(X) * .8)
q_90 = int(len(X) * .9)

X_train, y_train = np.array(X[:q_80]), np.array(Y[:q_80])
X_test, y_test = np.array(X[q_80:]), np.array(Y[q_80:])

import matplotlib.pyplot as plt

def flatten(lst):
    flattened = []
    for item in lst:
        if isinstance(item, (list,np.ndarray)):
            flattened.extend(flatten(item))
        else:
            flattened.append(item)
    return np.array(flattened)

def calculate_sign_percentage(arr1, arr2):
    if len(arr1) != len(arr2):
        raise ValueError("Arrays must be of equal length")

    subs = [[abs(x-y) for x,y in zip(a,b)] for a,b in zip(arr1,arr2)]

    avg_single = [sum(x)/len(x) for x in subs]

    subs = flatten(subs)
    avg_err = sum(subs)/len(subs)

    min_sub = min(subs)
    max_sub = max(subs)

    i_min = int(np.argmin(subs)/10)
    i_max = int(np.argmax(subs)/10)

    # min_max = [arr1[i_min],arr2[i_min],arr1[i_max],arr2[i_max]]

    return avg_err,min_sub,max_sub,avg_single

from sklearn.preprocessing import MinMaxScaler
import numpy as np

f_scaler = MinMaxScaler(feature_range=(-1, 1))

X_train = f_scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)
X_test = f_scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)

f_y_scaler = MinMaxScaler(feature_range=(-1, 1))
y_train = f_y_scaler.fit_transform(y_train)
y_testt = y_test
print(y_testt)
y_test = f_y_scaler.transform(y_test)

from sklearn.linear_model import LinearRegression

lr = LinearRegression()
lr.fit(X_train,y_train)
y_pred_lr = f_y_scaler.inverse_transform(lr.predict(X_test))
xt_lr = pd.DataFrame(X_test).iloc[:, 49].values
yp_lr = pd.DataFrame(y_pred_lr).iloc[:, 39].values
yt_lr = pd.DataFrame(y_test).iloc[:, 39].values

trueChange_lr = ( ( yt_lr - xt_lr ) * 100 ) / xt_lr
predChange_lr = ( ( yp_lr - xt_lr ) * 100 ) / xt_lr

print(y_pred_lr)

avg_err,min_sub,max_sub,avg_single = calculate_sign_percentage(y_testt, y_pred_lr)
plt.plot(avg_single)
plt.show()

print(f"\n\nAverage Error : {avg_err}")
print(f"Minimum Error : {min_sub}")
print(f"Maximum Error : {max_sub}")

from sklearn.ensemble import RandomForestRegressor

rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)
y_pred_rf = f_y_scaler.inverse_transform(rf_model.predict(X_test))

xt_rf = pd.DataFrame(X_test).iloc[:, 49].values
yp_rf = pd.DataFrame(y_pred_rf).iloc[:, 39].values
yt_rf = pd.DataFrame(y_test).iloc[:, 39].values

trueChange_rf = ( ( yt_rf - xt_rf )*100 ) / xt_rf
predChange_rf = ( ( yp_rf - xt_rf )*100 ) / xt_rf

print(y_pred_rf.shape)

avg_err,min_sub,max_sub,avg_single = calculate_sign_percentage(y_testt, y_pred_rf)
plt.plot(avg_single)
plt.show()

print(f"\n\nAverage Error : {avg_err}")
print(f"Minimum Error : {min_sub}")
print(f"Maximum Error : {max_sub}")

from sklearn.svm import SVR
from sklearn.multioutput import MultiOutputRegressor


svr = MultiOutputRegressor(SVR())

svr.fit(X_train, y_train)

y_pred_svr = f_y_scaler.inverse_transform(svr.predict(X_test))

xt_svr = pd.DataFrame(X_test).iloc[:, 49].values
yp_svr = pd.DataFrame(y_pred_svr).iloc[:, 39].values
yt_svr = pd.DataFrame(y_test).iloc[:, 39].values

trueChange_svr = ( ( yt_svr - xt_svr ) * 100 ) / xt_svr
predChange_svr = ( ( yp_svr - xt_svr ) * 100 ) / xt_svr

avg_err,min_sub,max_sub,avg_single = calculate_sign_percentage(y_testt, y_pred_svr)
plt.plot(avg_single)
plt.show()

print(f"\n\nAverage Error : {avg_err}")
print(f"Minimum Error : {min_sub}")
print(f"Maximum Error : {max_sub}")

avg_err,min_sub,max_sub,avg_single = calculate_sign_percentage(y_testt, y_pred_svr)
plt.plot(avg_single)
plt.show()

print(f"\n\nAverage Error : {avg_err}")
print(f"Minimum Error : {min_sub}")
print(f"Maximum Error : {max_sub}")

xt = np.array(pd.DataFrame(f_scaler.inverse_transform(X_test)).iloc[:, 49].values)
yp_svr = np.array(pd.DataFrame(y_pred_svr).iloc[:, :49].values)
yp_rf = np.array(pd.DataFrame(y_pred_rf).iloc[:, :49].values)
yp_lr = np.array(pd.DataFrame(y_pred_lr).iloc[:, :49].values)
yt = np.array(pd.DataFrame(f_y_scaler.inverse_transform(y_test)).iloc[:, :49].values)


print(xt[0])
print(yp_svr[0])



# Calculate the differences using broadcasting
change_pr_svr = xt[:, np.newaxis] - yp_svr


change_pr_rf = xt[:, np.newaxis] - yp_rf


change_pr_svr = xt[:, np.newaxis] - yp_lr

change_real  = xt[:, np.newaxis] - yt

# Example lists
list1 = yp_svr
list2 = yp_rf
list3 = yp_lr


combined_list = []

# Iterate over each sublist in the original lists
for sublist1, sublist2, sublist3 in zip(list1, list2, list3):
    combined_sublist = []
    # Combine elements from each sublist into one sublist with three elements
    for i in range(len(sublist1)):
        combined_sublist.append([sublist1[i], sublist2[i], sublist3[i]])
    combined_list.append(combined_sublist)

print(combined_list)
combined_list = np.array(combined_list)

from keras.models import Sequential
from keras.layers import LSTM, Dense

# Initialize the model
model = Sequential()
model.add(LSTM(50, return_sequences=True, input_shape=(49, 3)))  # input_shape matches the time steps and features
model.add(LSTM(50))
model.add(Dense(49, activation='sigmoid'))  # Output layer with 27 units for combining all the data

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Model summary
model.summary()

from sklearn.model_selection import train_test_split
# Split the data
X_train, X_test, y_train, y_test = train_test_split(combined_list, yt, test_size=0.2, random_state=42,shuffle=False)
X_train, _, y_train, _ = train_test_split(combined_list, yt, test_size=0.2, random_state=42)

scaler = MinMaxScaler(feature_range=(0, 1))

X_train_scaled = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)
X_test_scaled = scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)

y_scaler = MinMaxScaler(feature_range=(0, 1))
y_train_scaled = y_scaler.fit_transform(y_train)
y_test_scaled = y_scaler.transform(y_test)

print(X_train_scaled)

# Train the model
history = model.fit(X_train_scaled, y_train_scaled, epochs=25, validation_data=(X_test_scaled, y_test_scaled))

# Evaluate the model
performance = model.evaluate(X_test_scaled, y_test_scaled)
print(f'Test Loss: {performance[0]}, Test Accuracy: {performance[1]}')

final_model_predicted_scaled = model.predict(X_test_scaled)
final_model_predicted = y_scaler.inverse_transform(final_model_predicted_scaled)

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
mse = mean_squared_error(y_test, final_model_predicted)
print(f"Mean Squared Error (MSE): {mse}")

# Calculate Mean Absolute Error (MAE)
mae = mean_absolute_error(y_test, final_model_predicted)
print(f"Mean Absolute Error (MAE): {mae}")

# Calculate R-squared (R²) Score
r2 = r2_score(y_test, final_model_predicted)
print(f"R-squared (R²) Score: {r2}")

print(final_model_predicted)
pred_ml = []
for i in final_model_predicted[0]:
  pred_ml.append(i)
print()

avg_err,min_sub,max_sub,avg_single = calculate_sign_percentage(y_test, final_model_predicted)
plt.plot(avg_single)
plt.show()

print(f"\n\nAverage Error : {avg_err}")
print(f"Minimum Error : {min_sub}")
print(f"Maximum Error : {max_sub}")

with open('/content/cosine_similarity.py') as file:
  exec(file.read())

cs_df['date_column'] = cs_df.index
cs_df = cs_df.drop('close',axis=1)
cs_df.to_csv('cosine_similarity.csv')

import pandas as pd

original_data = pd.read_csv("./tvdatafeed_data.csv")
original_data_columns = original_data.columns

# Create a dictionary with the lists
data = {
  original_data_columns[0]: original_data[original_data_columns[0]],
  original_data_columns[1]: original_data[original_data_columns[1]],
  original_data_columns[2]: original_data[original_data_columns[2]],
  original_data_columns[3]: original_data[original_data_columns[3]],
  original_data_columns[4]: original_data[original_data_columns[4]],
  original_data_columns[5]: original_data[original_data_columns[5]],
  original_data_columns[6]: original_data[original_data_columns[6]],
}

# Create a DataFrame from the dictionary
df = pd.DataFrame(dict([(k, pd.Series(v)) for k, v in data.items()]))

# Write the DataFrame to a CSV file
df.to_csv('output.csv', index=False)

print("CSV file written successfully.")

pred_data = {
  'ID': range(1, len(final_model_predicted[-1]) + 1),
  'pred_ml': final_model_predicted[-1],
  'pred_dl': last_model[-1],
}

df = pd.DataFrame(dict([(k, pd.Series(v)) for k, v in pred_data.items()]))

if period == Interval.in_1_hour:
  df.to_csv(f'NEW_H_{pair}.csv', index=False)
else:
  df.to_csv(f'NEW_D_{pair}.csv', index=False)

print("CSV file written successfully.")

# Read the first CSV file into a DataFrame
df1 = pd.read_csv('cosine_similarity.csv')

# Read the second CSV file into a DataFrame
df2 = pd.read_csv('output.csv')

# Merge the DataFrames based on the common column 'key_column'
merged_df = pd.merge(df1, df2, on='date_column', how='inner')  # You can change 'inner' to 'outer', 'left', or 'right' as needed
merged_df = merged_df.drop('date_column',axis=1)
merged_df['ID'] = range(1, len(df1) + 1)

if period == Interval.in_1_hour:
  name = f'OLD_H_{pair}.csv'
else:
  name = f'OLD_D_{pair}.csv'

# Save the merged DataFrame to a new CSV file
if period == Interval.in_1_hour:
  merged_df.to_csv(name, index=False)
else:
  merged_df.to_csv(name, index=False)

print("CSV files merged and saved to merged_file.csv successfully.")

import pandas as pd

# Read the CSV file into a DataFrame
df = pd.read_csv(name)

# Convert the datetime column to pandas datetime
df['datetime'] = pd.to_datetime(df['datetime'])

# Function to remove the date part except for 1 AM records
def remove_date_except_1am(row):
  if row['datetime'].time().hour != 1:
    return row['datetime'].time().strftime('%H')
  else:
    return row['datetime'].strftime('%Y-%m-%d %H')

def remove_hours(row):
  return row['datetime'].strftime('%Y-%m-%d')

# Apply the function to the datetime column
if period != Interval.in_daily:
  df['datetime'] = df.apply(remove_date_except_1am, axis=1)
else:
  df['datetime'] = df.apply(remove_hours, axis=1)

# Save the updated DataFrame to a new CSV file
df.to_csv(name, index=False)

print("Date removed from non-1 AM records successfully.")